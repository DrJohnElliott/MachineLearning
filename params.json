{
  "name": "John Elliott Class Project for Machine Learning",
  "tagline": "Coursera Data Science ",
  "body": "# Machine Learning \r\n# Final Class Project\r\n# John Elliott\r\n\r\n### September 3, 2016\r\n\r\n## **Background of the project:**\r\n\r\nThis work is created for the final class project in the Practical Machine Learning Course, as part of the Coursera Data Science Track. To demonstrate machine learning techniques learned during the course data from http://groupware.les.inf.puc-rio.br/har is used to construct a model that can discern between the outcome variable “classe”. The data is from fitbit type devices worn by weightlifters performing exercises where they purposely performed the exercise with a particular type of fault which makes up the classe variable. The data experiment was published in the following paper:\r\n\r\nVelloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human ’13) . Stuttgart, Germany: ACM SIGCHI, 2013. and you can read more about it here: http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises\r\n\r\n## **Executive summary of the results:**\r\n\r\nTo predict the outcome of the testing data, 3 models were developed. The first model, “Model 1” was a Classification Tree that used all predictor variables. It was not able to predict one of the classe type, “D”. Therefore another Classification Tree model was created, “Model 2” by sub-setting predictor variables by selecting them from feature plots where separation could be visually seen. Model 2 was able to predict all classe types but had slightly lower accuracy compared to Model 1. The performance of the two classification trees was disappointing, both with accuracy below 60%. A third model was created “Model 3” using a Random Forest method. This model yielded excellent performance with the data, with an accuracy of 99% and was able to predict all outcomes of the test data accurately.\r\n\r\n## **Code:**\r\n\r\n### **Data Download:**\r\n\r\nData is downloaded from the defined URLs. The code makes a check to see if the required data file is present in the working directory. If it is not found then the file is downloaded and placed into the working directory.\r\n\r\n    `URL <- c(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\",`\r\n    `\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\")`\r\n    `required_files <- c(\"pml-training.csv\", \"pml-testing.csv\")`\r\n       \r\n        `for(i in 1:length(URL)){`\r\n                `if(!all(lapply(required_files[i],file.exists)==TRUE)){ `\r\n                        `print(\"Required Data Files Not Found!\")`\r\n                        `print(\"Please wait while the files are downloaded\")`\r\n                        `download.file(URL[i], destfile = required_files[i], mode=\"wb\")`\r\n                `}`\r\n        `}`\r\n\r\nLoad Data Into Envioroment (Reading In Data)\r\n\r\nA check for the required data is made and if not found it is read into the environment. Also “cache = TRUE” is set for the data chunk to provide data caching.\r\n\r\n        `if(!exists(\"trainingData\")){ `\r\n                `print(\"Please Wait while dataset is loaded\")`\r\n                `trainingData <- read.csv(\"pml-training.csv\", header = TRUE, sep = \",\" ,`\r\n                                `dec = \".\",na.strings = c(\"\", NA) )`\r\n        `}`\r\n\r\n        `if(!exists(\"testingData\")){ `\r\n                `print(\"Please Wait while dataset is loaded\")`\r\n                `testingData <-  read.csv(\"pml-testing.csv\", header = TRUE, sep = \",\" ,`\r\n                                `dec = \".\", na.strings = c(\"\", NA) )`\r\n        `}`\r\n\r\n## **Inspection of the data set**\r\n\r\nFirst we take a quick look at the data set to see what we have.\r\n\r\n        `dim(testingData)`\r\n        `head(testingData[ 3,1:5])`\r\n        `str(testingData[3, 1:5])  `\r\n        `head(trainingData[ 3,1:5])`\r\n        `str(trainingData[3, 1:5])`\r\n\r\n***     \r\n> [1]  20 160\r\n>   X user_name raw_timestamp_part_1 raw_timestamp_part_2   cvtd_timestamp\r\n> 3 3    jeremy           1322673075               342967 30/11/2011 17:11\r\n\r\n***\r\n\r\n> 'data.frame':   1 obs. of  5 variables:\r\n>  $ X                   : int 3\r\n>  $ user_name           : Factor w/ 6 levels \"adelmo\",\"carlitos\",..: 5\r\n>  $ raw_timestamp_part_1: int 1322673075\r\n>  $ raw_timestamp_part_2: int 342967\r\n>  $ cvtd_timestamp      : Factor w/ 11 levels \"02/12/2011 13:33\",..: 10\r\n\r\n***\r\n\r\n>   X user_name raw_timestamp_part_1 raw_timestamp_part_2   cvtd_timestamp\r\n> 3 3  carlitos           1323084231               820366 05/12/2011 11:23\r\n\r\n***\r\n\r\n> 'data.frame':   1 obs. of  5 variables:\r\n>  $ X                   : int 3\r\n>  $ user_name           : Factor w/ 6 levels \"adelmo\",\"carlitos\",..: 2\r\n>  $ raw_timestamp_part_1: int 1323084231\r\n>  $ raw_timestamp_part_2: int 820366\r\n>  $ cvtd_timestamp      : Factor w/ 20 levels \"02/12/2011 13:32\",..: 9\r\n\r\n***\r\n\r\nWe check for incomplete data columns with Na’s in the two data sets, remove incomplete variables from the population and omit first 7 columns of descriptor variables.\r\n\r\n    `testingData     <- testingData[, colSums(is.na(testingData)) == 0] `\r\n    `trainingData    <- trainingData[, colSums(is.na(trainingData)) == 0] `\r\n    `checknames      <- is.na(match(colnames(trainingData), colnames(testingData)))`\r\n    `trainingData    <- trainingData[,7:60]`\r\n    `testingData     <- testingData[,7:60]`\r\n\r\nWe create a validation subset from the training data\r\n\r\n        `set.seed(1972) `\r\n    `inTrain         <- createDataPartition(trainingData$classe, p = 0.7, list = FALSE)`\r\n    `trainingData    <- trainingData[inTrain, ]`\r\n    `validate        <- trainingData[-inTrain, ]`\r\n\r\n\r\n## **Exploratory plots**\r\n\r\nFeature plots of the data are created to look for a subset of predictor variables. An example plot of 4 variables is shown, the process was repeated to find predictors that may improve the modeling accuracy. The list of predictors was placed in a variable named training_set.\r\n\r\n       `featurePlot(x = trainingData[, 10:13], `\r\n            `y = trainingData$classe,`\r\n            `plot = \"ellipse\",`\r\n            `## Add a key at the top`\r\n            `auto.key = list(columns = 3))`\r\n\r\n![Feature Plot of predictors](https://github.com/DrJohnElliott/MachineLearning/blob/master/Rplot.png?raw=TRUE)\r\n\r\n\r\n# **Model development of Classification Trees:**\r\n\r\n## **Model One:**\r\n\r\nThe first model is a classification tree (method = “rpart”) using all predictors. The model uses the default train control method of resampling, cross validation “cv”, and a fold of 10.\r\n\r\n       ` set.seed(1972)`\r\n    `model_1         <- train(as.factor(classe) ~ . ,  method = \"rpart\", data = trainingData)`\r\n        `fancyRpartPlot(model_1$finalModel)`\r\n\r\n![Model 1: Classification Tree](https://github.com/DrJohnElliott/MachineLearning/blob/master/Mod_1.png?raw=true )\r\n\r\nThe first model was not able to discern classe type “D”, the alternate set of predictors were used to create the second model a pruned version of the first classification tree.\r\n\r\n## **Model Two:**\r\n\r\nModel two also uses the default train control method of resampling, cross validation “cv”, and a fold of 10.\r\n\r\n    `model_2         <- train(as.factor(classe) ~ accel_belt_y + accel_belt_z + magnet_belt_x + magnet_belt_y + accel_dumbbell_z + magnet_dumbbell_x + magnet_dumbbell_y + magnet_dumbbell_z + roll_forearm + pitch_forearm + yaw_forearm + total_accel_forearm + gyros_forearm_x , method = \"rpart\", data = trainingData)`        \r\n        `fancyRpartPlot(model_2$finalModel)`\r\n\r\n![Model 2: Clasification Tree with sub-setted predictors](https://github.com/DrJohnElliott/MachineLearning/blob/master/Mod_2.png?raw=true)\r\n\r\n## **Validation of the Classification Trees**\r\n\r\nValidation of the two models was performed using the validation data that was sub-setted from the training data.\r\n\r\n    `mod1_prediction <- predict(model_1, validate)`\r\n    `mod2_prediction <- predict(model_2, validate)`\r\n    `mod1_confusion  <- as.matrix(confusionMatrix(validate$classe, mod1_prediction))`\r\n    `mod2_confusion  <- as.matrix(confusionMatrix(validate$classe, mod2_prediction))`\r\n    `accuracy_mod1   <- confusionMatrix(validate$classe, mod1_prediction)$overall[1]`\r\n    `accuracy_mod2   <- confusionMatrix(validate$classe, mod2_prediction)$overall[1]`\r\n    `Predictions made with Classification Trees:`\r\n    `mod1_predict    <- predict(model_1, testingData)`\r\n    `mod2_predict    <- predict(model_2, testingData)`\r\n\r\nModel 1 has better accuracy at 0.5661337 compared to model 2 accuracy of 0.4779554 . With both models having accuracy below 60% another modelling method was explored.\r\n\r\n## **Model development of Random Forests**\r\n\r\nThe last model explored is a Random Forest (method = “rf”) using all predictor variables.\r\n\r\n## **Model Three:**\r\n\r\nFor the training The train control used was k fold cross validation “cv”, with a k fold value of 5.\r\n\r\n    `myControl       <- trainControl(method = \"cv\", number = 5)`\r\n    `mod3            <- train(as.factor(classe) ~ .,method=\"rf\",trControl=myControl,data=trainingData)`\r\n\r\n## **Validation of the Random Forest**\r\n\r\nValidation of the Random Forest model was performed using the validation data that was sub-setted from the training data.\r\n\r\n    `val_predict     <- predict(mod3, validate)`\r\n    `My_Confusion    <- as.matrix(confusionMatrix(validate$classe, val_predict))`\r\n    `accuracy_mod3   <- confusionMatrix(validate$classe, val_predict)$overall[1]`  \r\n\r\nThe Random Forest model is able to predict all classe types and has an accuracy approaching 100% based on the validation data.\r\n\r\n    `mod3_predict    <- predict(mod3, testingData)`\r\n\r\n## **Model Comparison**\r\n\r\nTables of the accuracy and prediction values are created for model comparison.\r\n\r\n    `Accuracy_Table          <- as.data.frame(matrix(nrow=3,ncol=2))`\r\n    `Method                  <- c(\"Classification Tree\", \"Sub-setted Tree\", \"Random Forest\")`\r\n    `rownames(Accuracy_Table)<-c(\"Model 1\", \"Model 2\", \"Model 3\")`\r\n    `colnames(Accuracy_Table)<- c( \"Method Used\", \"Accuracy\")`\r\n    `Accuracy_Table[,1]      <- as.data.frame(Method)`\r\n    `Accuracy_Table[1,2]     <- as.data.frame(accuracy_mod1)`\r\n    `Accuracy_Table[2,2]     <- as.data.frame(accuracy_mod2)`\r\n    `Accuracy_Table[3,2]     <- as.data.frame(accuracy_mod3)`\r\n    `Predict_Table           <- as.data.frame(matrix(nrow=3,ncol=20))`\r\n    `rownames(Predict_Table) <-c(\"Model 1\", \"Model 2\", \"Model 3\")`\r\n    `Predict_Table[1, ]       <- mod1_predict`\r\n    `Predict_Table[2, ]       <- mod2_predict`\r\n    `Predict_Table[3, ]       <- mod3_predict`\r\n    `(Accuracy_Table)`\r\n\r\n\r\n***\r\n`                Method Used             Accuracy`\r\n\r\n    `Model 1     Classification Tree     0.5661337`\r\n\r\n    `Model 2     Sub-setted Tree         0.4779554`\r\n\r\n    `Model 3     Random Forest           1.0000000`\r\n\r\n***\r\n\r\n    `(Predict_Table)`\r\n\r\n***\r\n         \r\n     `Model 1    A  A  C  A  A  C  C  C  A   A   C   C   C   A   C   C   A   A   A   C`\r\n\r\n     `Model 2    C  A  D  A  A  C  D  A  A   A   C   D   C   A   D   A   E   A   A   D`\r\n\r\n     `Model 3    B  A  B  A  A  E  D  B  A   A   B   C   B   A   E   E   A   B   B   B`\r\n***\r\n\r\n## Plots of the Confusion Matrix for each model\r\n\r\n    `library(reshape2)`\r\n    `melted_mod1_confusion   <- melt(mod1_confusion)`\r\n    `melted_mod2_confusion   <- melt(mod2_confusion)`\r\n    `melted_My_Confusion     <- melt(My_Confusion)`\r\n    `colnames( melted_mod1_confusion ) <- c(\"Classe\", \"Predicted\",\"value\")`\r\n    `colnames( melted_mod2_confusion ) <- c(\"Classe\", \"Predicted\",\"value\")`\r\n    `colnames( melted_My_Confusion ) <- c(\"Classe\", \"Predicted\",\"value\")`\r\n    `library(gridExtra)`\r\n    `g1 <-ggplot(data = melted_mod1_confusion, aes(x=Classe, y=Predicted, fill=value)) + `\r\n    `geom_tile() + ggtitle(\"Model 1\") + coord_fixed(ratio=1)`\r\n    `g2 <-ggplot(data = melted_mod2_confusion, aes(x=Classe, y=Predicted, fill=value)) + `\r\n    `geom_tile() + ggtitle(\"Model 2\") + coord_fixed(ratio=1)`\r\n    `g3 <- ggplot(data = melted_My_Confusion, aes(x=Classe, y=Predicted, fill=value)) + `\r\n    `geom_tile() + ggtitle(\"Model 3\") + coord_fixed(ratio=1)`\r\n    `grid.arrange(g1, g2, g3, ncol = 3)`\r\n\r\n![Confusion Matrices of the Models](https://github.com/DrJohnElliott/MachineLearning/blob/master/confusionMatices.png?raw=TRUE)\r\n\r\nThe plots above of the confusion matrices, show how the Random Forest model performance (model 3) out performs the Classification Tree model. It can also be seen that Model 1 can not predict Classe type “D”.\r\n\r\n# **Summary and Conclusions:**\r\n\r\nThe random Forest model out performed the Classification Tree and Pruned tree models by leaps and bounds with an accuracy over 99%, concluding for this data set it is the best model selection. An important note is that the results are only based on the data set that the models were trained on and it is expected the results could be different for data collected under different circumstances and conditions, such as new individuals who perform the exercises.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}