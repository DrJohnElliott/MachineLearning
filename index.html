<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Machinelearning by DrJohnElliott</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Machinelearning</h1>
      <h2 class="project-tagline">Coursera Data Science </h2>
      <a href="https://github.com/DrJohnElliott/MachineLearning" class="btn">View on GitHub</a>
      <a href="https://github.com/DrJohnElliott/MachineLearning/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/DrJohnElliott/MachineLearning/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <p>Machine Learning Final Class Project
John Elliott</p>

<p>September 3, 2016</p>

<p>Background of the project:
This work is created for the final class project in the Practical Machine Learning Course, as part of the Coursera Data Science Track. To demonstrate machine learning techniques learned during the course data from <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a> is used to construct a model that can discern between the outcome variable “classe”. The data is from fitbit type devices worn by weightlifters performing exercises where they purposely performed the exercise with a particular type of fault which makes up the classe variable. The data experiment was published in the following paper:</p>

<p>Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human ’13) . Stuttgart, Germany: ACM SIGCHI, 2013. and you can read more about it here: <a href="http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises">http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises</a></p>

<p>Executive summary of the results:
To predict the outcome of the testing data, 3 models were developed. The first model, “Model 1” was a Classification Tree that used all predictor variables. It was not able to predict one of the classe type, “D”. Therefore another Classification Tree model was created, “Model 2” by sub-setting predictor variables by selecting them from feature plots where separation could be visually seen. Model 2 was able to predict all classe types but had slightly lower accuracy compared to Model 1. The performance of the two classification trees was disappointing, both with accuracy below 60%. A third model was created “Model 3” using a Random Forest method. This model yielded excellent performance with the data, with an accuracy of 99% and was able to predict all outcomes of the test data accurately.</p>

<p>Code:
Data Download:</p>

<p>Data is downloaded from the defined URLs. The code makes a check to see if the required data file is present in the working directory. If it is not found then the file is downloaded and placed into the working directory.</p>

<pre><code>    URL &lt;- c("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
             "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")

    required_files &lt;- c("pml-training.csv", "pml-testing.csv")

    for(i in 1:length(URL)){
            if(!all(lapply(required_files[i],file.exists)==TRUE)){ 
                    print("Required Data Files Not Found!")
                    print("Please wait while the files are downloaded")
                    download.file(URL[i], destfile = required_files[i], mode="wb")
            }
    }
</code></pre>

<p>Load Data Into Envioroment (Reading In Data)</p>

<p>A check for the required data is made and if not found it is read into the environment. Also “cache = TRUE” is set for the data chunk to provide data caching.</p>

<pre><code>    if(!exists("trainingData")){ 
            print("Please Wait while dataset is loaded")
            trainingData &lt;- read.csv("pml-training.csv", header = TRUE, sep = "," ,
                            dec = ".",na.strings = c("", NA) )
    }
</code></pre>

<p>[1] "Please Wait while dataset is loaded"
        if(!exists("testingData")){ 
                print("Please Wait while dataset is loaded")
                testingData &lt;-  read.csv("pml-testing.csv", header = TRUE, sep = "," ,
                                dec = ".", na.strings = c("", NA) )
        }
[1] "Please Wait while dataset is loaded"
Inspection of the dataset</p>

<p>First we take a quick look at the data set to see what we have.</p>

<p>[1]  20 160
  X user_name raw_timestamp_part_1 raw_timestamp_part_2   cvtd_timestamp
3 3    jeremy           1322673075               342967 30/11/2011 17:11
'data.frame':   1 obs. of  5 variables:
 $ X                   : int 3
 $ user_name           : Factor w/ 6 levels "adelmo","carlitos",..: 5
 $ raw_timestamp_part_1: int 1322673075
 $ raw_timestamp_part_2: int 342967
 $ cvtd_timestamp      : Factor w/ 11 levels "02/12/2011 13:33",..: 10
  X user_name raw_timestamp_part_1 raw_timestamp_part_2   cvtd_timestamp
3 3  carlitos           1323084231               820366 05/12/2011 11:23
'data.frame':   1 obs. of  5 variables:
 $ X                   : int 3
 $ user_name           : Factor w/ 6 levels "adelmo","carlitos",..: 2
 $ raw_timestamp_part_1: int 1323084231
 $ raw_timestamp_part_2: int 820366
 $ cvtd_timestamp      : Factor w/ 20 levels "02/12/2011 13:32",..: 9
We check for incomplete data columns with Na’s in the two data sets, remove incomplete variables from the population and omit first 7 columns of descriptor variables.</p>

<p>testingData     &lt;- testingData[, colSums(is.na(testingData)) == 0] 
trainingData    &lt;- trainingData[, colSums(is.na(trainingData)) == 0] </p>

<p>checknames      &lt;- is.na(match(colnames(trainingData), colnames(testingData)))</p>

<p>trainingData    &lt;- trainingData[,7:60]
testingData     &lt;- testingData[,7:60]<br>
We create a validation subset from the training data</p>

<pre><code>    set.seed(1972) 
</code></pre>

<p>inTrain         &lt;- createDataPartition(trainingData$classe, p = 0.7, list = FALSE)
trainingData    &lt;- trainingData[inTrain, ]
validate        &lt;- trainingData[-inTrain, ]
Exploratory plots of the data are created to look for a subset of predictor variables. An example plot of 4 variables is shown, the process was repeated to find predictors that may improve the modeling accuracy. The list of predictors was placed in a variable named training_set.</p>

<p>Model development of Classification Trees:</p>

<p>Model One:</p>

<p>The first model is a classification tree (method = “rpart”) using all predictors. The model uses the default train control method of resampling, cross validation “cv”, and a fold of 10.</p>

<pre><code>    set.seed(1972)
</code></pre>

<p>model_1         &lt;- train(as.factor(classe) ~ . ,  method = "rpart", data = trainingData)</p>

<pre><code>    fancyRpartPlot(model_1$finalModel)
</code></pre>

<p><img src="https://github.com/DrJohnElliott/MachineLearning/blob/master/Mod_1.png?raw=true" alt="Model 1: Classification Tree"></p>

<p>The first model was not able to discern classe type “D”, the alternate set of predictors were used to create the second model a pruned version of the first classification tree.</p>

<p>Model Two:</p>

<p>Model two also uses the default train control method of resampling, cross validation “cv”, and a fold of 10.</p>

<p>model_2         &lt;- train(as.factor(classe) ~ accel_belt_y + accel_belt_z + magnet_belt_x + magnet_belt_y + accel_dumbbell_z + magnet_dumbbell_x + magnet_dumbbell_y + magnet_dumbbell_z + roll_forearm + pitch_forearm + yaw_forearm + total_accel_forearm + gyros_forearm_x , method = "rpart", data = trainingData)</p>

<pre><code>    fancyRpartPlot(model_2$finalModel)
</code></pre>

<p><img src="https://github.com/DrJohnElliott/MachineLearning/blob/master/Mod_2.png?raw=true" alt="Model 2: Clasification Tree with sub-setted predictors"></p>

<p>Validation of the Classification Trees</p>

<p>Validation of the two models was performed using the validation data that was sub-setted from the training data.</p>

<p>mod1_prediction &lt;- predict(model_1, validate)
mod2_prediction &lt;- predict(model_2, validate)</p>

<p>mod1_confusion  &lt;- as.matrix(confusionMatrix(validate$classe, mod1_prediction))
mod2_confusion  &lt;- as.matrix(confusionMatrix(validate$classe, mod2_prediction))</p>

<p>accuracy_mod1   &lt;- confusionMatrix(validate$classe, mod1_prediction)$overall[1]
accuracy_mod2   &lt;- confusionMatrix(validate$classe, mod2_prediction)$overall[1]
Predictions made with Classification Trees:</p>

<p>mod1_predict    &lt;- predict(model_1, testingData)
mod2_predict    &lt;- predict(model_2, testingData)
Model 1 has better accuracy at 0.5661337 compared to model 2 accuracy of 0.4779554 . With both models having accuracy below 60% another modelling method was explored.</p>

<p>Model development of Random Forests</p>

<p>The last model explored is a Random Forest (method = “rf”) using all predictor variables.</p>

<p>Model Three:</p>

<p>For the training The train control used was k fold cross validation “cv”, with a k fold value of 5.</p>

<p>myControl       &lt;- trainControl(method = "cv", number = 5)</p>

<p>mod3            &lt;- train(as.factor(classe) ~ .,method="rf",trControl=myControl,data=trainingData)
Validation of the Random Forest</p>

<p>Validation of the Random Forest model was performed using the validation data that was sub-setted from the training data.</p>

<p>val_predict     &lt;- predict(mod3, validate)
My_Confusion    &lt;- as.matrix(confusionMatrix(validate$classe, val_predict))
accuracy_mod3   &lt;- confusionMatrix(validate$classe, val_predict)$overall[1]<br>
The Random Forest model is able to predict all classe types and has an accuracy approaching 100% based on the validation data.</p>

<p>mod3_predict    &lt;- predict(mod3, testingData)
Model Comparison</p>

<p>Tables of the accuracy and prediction values are created for model comparison.</p>

<p>Accuracy_Table          &lt;- as.data.frame(matrix(nrow=3,ncol=2))
Method                  &lt;- c("Classification Tree", "Sub-setted Tree", "Random Forest")
rownames(Accuracy_Table)&lt;-c("Model 1", "Model 2", "Model 3")
colnames(Accuracy_Table)&lt;- c( "Method Used", "Accuracy")
Accuracy_Table[,1]      &lt;- as.data.frame(Method)
Accuracy_Table[1,2]     &lt;- as.data.frame(accuracy_mod1)
Accuracy_Table[2,2]     &lt;- as.data.frame(accuracy_mod2)
Accuracy_Table[3,2]     &lt;- as.data.frame(accuracy_mod3)</p>

<p>Predict_Table           &lt;- as.data.frame(matrix(nrow=3,ncol=20))
rownames(Predict_Table) &lt;-c("Model 1", "Model 2", "Model 3")
Predict_Table[1, ]       &lt;- mod1_predict
Predict_Table[2, ]       &lt;- mod2_predict
Predict_Table[3, ]       &lt;- mod3_predict</p>

<p>(Accuracy_Table)
                Method Used  Accuracy
Model 1 Classification Tree 0.5661337
Model 2     Sub-setted Tree 0.4779554
Model 3       Random Forest 1.0000000</p>

<p>(Predict_Table)
        V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20
Model 1  A  A  C  A  A  C  C  C  A   A   C   C   C   A   C   C   A   A   A   C
Model 2  C  A  D  A  A  C  D  A  A   A   C   D   C   A   D   A   E   A   A   D
Model 3  B  A  B  A  A  E  D  B  A   A   B   C   B   A   E   E   A   B   B   B</p>

<p>Plots of the Confusion Matrix for each model</p>

<p>library(reshape2)
melted_mod1_confusion   &lt;- melt(mod1_confusion)
melted_mod2_confusion   &lt;- melt(mod2_confusion)
melted_My_Confusion     &lt;- melt(My_Confusion)
colnames( melted_mod1_confusion ) &lt;- c("Classe", "Predicted","value")
colnames( melted_mod2_confusion ) &lt;- c("Classe", "Predicted","value")
colnames( melted_My_Confusion ) &lt;- c("Classe", "Predicted","value")
library(gridExtra)
g1 &lt;-ggplot(data = melted_mod1_confusion, aes(x=Classe, y=Predicted, fill=value)) + 
  geom_tile() + ggtitle("Model 1") + coord_fixed(ratio=1)</p>

<p>g2 &lt;-ggplot(data = melted_mod2_confusion, aes(x=Classe, y=Predicted, fill=value)) + 
  geom_tile() + ggtitle("Model 2") + coord_fixed(ratio=1)</p>

<p>g3 &lt;- ggplot(data = melted_My_Confusion, aes(x=Classe, y=Predicted, fill=value)) + 
  geom_tile() + ggtitle("Model 3") + coord_fixed(ratio=1)</p>

<p>grid.arrange(g1, g2, g3, ncol = 3)</p>

<p>The plots above of the confusion matrices, show how the Random Forest model performance (model 3) out performs the Classification Tree model. It can also be seen that Model 1 can not predict Classe type “D”.</p>

<p>Summary and Conclusions
The random Forest model out performed the Classification Tree and Pruned tree models by leaps and bounds with an accuracy over 99%, concluding for this data set it is the best model selection. An important note is that the results are only based on the data set that the models were trained on and it is expected the results could be different for data collected under different circumstances and conditions, such as new individuals who perform the exercises.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/DrJohnElliott/MachineLearning">Machinelearning</a> is maintained by <a href="https://github.com/DrJohnElliott">DrJohnElliott</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
