---
title: "Elliott Machine Learning Final Class Project"
author: "John Elliott"
date: "September 3, 2016"
output: html_document
---

```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
        options(width = 100)
        library(knitr)
        opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

        options(xtable.type = 'html')
        
        knit_hooks$set(inline = function(x) {
                if(is.numeric(x)) {
                        round(x, getOption('digits'))
                } else {
                        paste(as.character(x), collapse = ', ')
                        }
                })
        knit_hooks$set(plot = knitr:::hook_plot_html)
```


```{r load_packages, echo=FALSE}
        setwd("C:/Users/John/Desktop/R-Code/Coursera/Machine Learning/Class Project")
        library(dplyr, quietly = TRUE,warn.conflicts = FALSE)
        library(ggplot2, quietly = TRUE,warn.conflicts = FALSE)
        library(lubridate, quietly = TRUE,warn.conflicts = FALSE)
        library(caret, quietly = TRUE,warn.conflicts = FALSE)
        library(rattle, quietly = TRUE,warn.conflicts = FALSE)
        library(AppliedPredictiveModeling, quietly = TRUE,warn.conflicts = FALSE)
        library(ElemStatLearn, quietly = TRUE,warn.conflicts = FALSE)
        library(pgmm, quietly = TRUE,warn.conflicts = FALSE)
        library(rpart, quietly = TRUE,warn.conflicts = FALSE)
        library(rpart.plot, quietly = TRUE,warn.conflicts = FALSE)
        
```

## Background of the project:

This work is created for the final class project in the Practical Machine Learning Course, as part of the Coursera Data Science Track. To demonstrate machine learning techniques learned during the course data from <http://groupware.les.inf.puc-rio.br/har> is used to construct a model that can discern between the different classe type. The data is from fitbit type devices worn by weightlifters performing exercises where they purposely performed the exercise with a particular type of fault winch makes up the **classe** variable.  The data experiment was published in the following paper: 
**Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.** and you can read more about it here: <http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises> 

## Execitive summary of the results:

To predict the outcome of the testing data, 3 models were developed. The first was a classification tree that used all predictor variables. It was not able to predict one of the classe types **"D"**. The classification tree was pruned by choosing predictor variables selected from feature plots where separation could be seen. The second model, the pruned classification tree was able to predict all classe types. The validation of the two classification trees was disappointing the first showed better accuracy but couldn't predict all classe types, both models had accuracy below 60%. A third model was created using a Random Forest method. This model yielded excellent results with the data set with an accuracy of 99%.


## Code:

####Data Download:
Data is downloaded from the defined URLs. The code makes a check to see if the required data file is present in the working directory. If it is not found then the file is downloaded and placed into the working directory. 
```{r  download_data, echo=TRUE }
        
        URL <- c("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                 "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")

        required_files <- c("pml-training.csv", "pml-testing.csv")

        for(i in 1:length(URL)){
                if(!all(lapply(required_files[i],file.exists)==TRUE)){ 
                        print("Required Data Files Not Found!")
                        print("Please wait while the files are downloaded")
                        download.file(URL[i], destfile = required_files[i], mode="wb")
                }
        }
        
        
```



#### Load Data Into Envioroment (Reading In Data)
A check for the required data is made and if not found it is read into the environment. 
Also "cache = TRUE" is set for the data chunk to provide data caching.
```{r load_data, echo=TRUE, cache=TRUE  }

        if(!exists("trainingData")){ 
                print("Please Wait while dataset is loaded")
                trainingData <- read.csv("pml-training.csv", header = TRUE, sep = "," ,
                                dec = ".",na.strings = c("", NA) )
        }

        if(!exists("testingData")){ 
                print("Please Wait while dataset is loaded")
                testingData <-  read.csv("pml-testing.csv", header = TRUE, sep = "," ,
                                dec = ".", na.strings = c("", NA) )
        }
```


#### Inspection of the dataset

First we take a quick look at the data set to see what we have.
```{r get_set_size, echo=FALSE}
        dim(testingData)
        head(testingData[ 3,1:5])
        str(testingData[3, 1:5])
        #colnames(testingData)
        
        head(trainingData[ 3,1:5])
        str(trainingData[3, 1:5])
        #colnames(trainingData)
         
```

We check for incomplete data columns with Na's in the two data sets, remove incomplete variables from the population and omit first 7 columns of descriptor variables.
```{r check_NAs, echo=TRUE}

testingData     <- testingData[, colSums(is.na(testingData)) == 0] 
trainingData    <- trainingData[, colSums(is.na(trainingData)) == 0] 

checknames      <- is.na(match(colnames(trainingData), colnames(testingData)))

trainingData    <- trainingData[,7:60]
testingData     <- testingData[,7:60]   
```

We create a validation subset from the training data
```{r validation, echo=TRUE}
     
        set.seed(1972) 

inTrain         <- createDataPartition(trainingData$classe, p = 0.7, list = FALSE)
trainingData    <- trainingData[inTrain, ]
validate        <- trainingData[-inTrain, ]

```        

Exploratory plots of the data are created to look for a subset of predictor variables. An example plot of 4 variables is shown, the process was repeated to find predictors that may improve the modeling accuracy. The list of predictors was placed in a variable named **training_set**.

```{r Plot_Data, echo=FALSE}
                                              
       featurePlot(x = trainingData[, 10:13], 
            y = trainingData$classe,
            plot = "ellipse",
            ## Add a key at the top
            auto.key = list(columns = 3))

training_set    <- colnames(trainingData[c(10:13,37:45)])

```

#### Model development of Classification Trees:

##### Model One:

The first model is a classification tree (method = "rpart") using all predictors. The model uses the default train control method of resampling, cross validation "cv", and a fold of 10.
```{r model_1, echo=TRUE, cache = TRUE }

        set.seed(1972)

model_1         <- train(as.factor(classe) ~ . ,  method = "rpart", data = trainingData)

        fancyRpartPlot(model_1$finalModel)
        
```

The first model was not able to discern classe type **"D"**, the alternate set of predictors were used to create the second model a pruned version of the first classification tree.

##### Model Two:
Model two also uses the default train control method of resampling, cross validation "cv", and a fold of 10.
```{r model_2, echo=TRUE, cache = TRUE}
model_2         <- train(as.factor(classe) ~ accel_belt_y + accel_belt_z + magnet_belt_x + magnet_belt_y + accel_dumbbell_z + magnet_dumbbell_x + magnet_dumbbell_y + magnet_dumbbell_z + roll_forearm + pitch_forearm + yaw_forearm + total_accel_forearm + gyros_forearm_x , method = "rpart", data = trainingData)

        
        fancyRpartPlot(model_2$finalModel)
        

```

#### Validation of the Classification Trees

Validation of the two models was performed using the validation data that was subset ted from the training data.

```{r, validation_1_2, echo=TRUE}

mod1_prediction <- predict(model_1, validate)
mod2_prediction <- predict(model_2, validate)

mod1_confusion  <- as.matrix(confusionMatrix(validate$classe, mod1_prediction))
mod2_confusion  <- as.matrix(confusionMatrix(validate$classe, mod2_prediction))

accuracy_mod1   <- confusionMatrix(validate$classe, mod1_prediction)$overall[1]
accuracy_mod2   <- confusionMatrix(validate$classe, mod2_prediction)$overall[1]

```

#### Predictions made with Classification Trees:

```{r, prediction_1_2, echo=TRUE}

mod1_predict    <- predict(model_1, testingData)
mod2_predict    <- predict(model_2, testingData)

```

Model 1 has better accuracy at `r accuracy_mod1` compared to model 2 accuracy of `r accuracy_mod2` . With both models having ac curacies below 60% another modelling method was explored.

#### Model development of Random Forests 

The last model explored is a Random Forest (method = "rf") using all predictor variables. 

##### Model Three:
For the training The train control used was k fold cross validation "cv", with a k fold value of 5. 
```{r model_3, echo=TRUE , cache=TRUE}

myControl       <- trainControl(method = "cv", number = 5)

mod3            <- train(as.factor(classe) ~ .,method="rf",trControl=myControl,data=trainingData)

```

#### Validation of the Random Forest

Validation of the Random Forest model was performed using the validation data that was subset ted from the training data.

```{r, validate_3, echo=TRUE}

val_predict     <- predict(mod3, validate)
My_Confusion    <- as.matrix(confusionMatrix(validate$classe, val_predict))
accuracy_mod3   <- confusionMatrix(validate$classe, val_predict)$overall[1]      
        

```

The Random Forest model is able to predict all classe types and has an accuracy approaching 100% based on the validation data. 

```{r, mod3_prediction, echo=TRUE}

mod3_predict    <- predict(mod3, testingData)

```


#### Model Comparison

Tables of the accuracy and prediction values are created for model comparison.
```{r, compare_models, echo=TRUE}
Accuracy_Table          <- as.data.frame(matrix(nrow=3,ncol=2))
Method                  <- c("Classification Tree", "Pruned Tree", "Random Forest")
rownames(Accuracy_Table)<-c("Model 1", "Model 2", "Model 3")
colnames(Accuracy_Table)<- c( "Method Used", "Accuracy")
Accuracy_Table[,1]      <- as.data.frame(Method)
Accuracy_Table[1,2]     <- as.data.frame(accuracy_mod1)
Accuracy_Table[2,2]     <- as.data.frame(accuracy_mod2)
Accuracy_Table[3,2]     <- as.data.frame(accuracy_mod3)

Predict_Table           <- as.data.frame(matrix(nrow=3,ncol=20))
rownames(Predict_Table) <-c("Model 1", "Model 2", "Model 3")
Predict_Table[1, ]       <- as.data.frame(mod1_predict)
Predict_Table[2, ]       <- as.data.frame(mod2_predict)
Predict_Table[3, ]       <- as.data.frame(mod3_predict)

(Accuracy_Table)
(Predict_Table)

```


#### Plots of the Confusion Matrix for each model
```{r, confusion_matrix, echo=TRUE}

library(reshape2)
melted_mod1_confusion   <- melt(mod1_confusion)
melted_mod2_confusion   <- melt(mod2_confusion)
melted_My_Confusion     <- melt(My_Confusion)
colnames( melted_mod1_confusion ) <- c("Classe", "Predicted","value")
colnames( melted_mod2_confusion ) <- c("Classe", "Predicted","value")
colnames( melted_My_Confusion ) <- c("Classe", "Predicted","value")
library(gridExtra)
g1 <-ggplot(data = melted_mod1_confusion, aes(x=Classe, y=Predicted, fill=value)) + 
  geom_tile() + ggtitle("Model 1") + coord_fixed(ratio=1)

g2 <-ggplot(data = melted_mod2_confusion, aes(x=Classe, y=Predicted, fill=value)) + 
  geom_tile() + ggtitle("Model 2") + coord_fixed(ratio=1)

g3 <- ggplot(data = melted_My_Confusion, aes(x=Classe, y=Predicted, fill=value)) + 
  geom_tile() + ggtitle("Model 3") + coord_fixed(ratio=1)

grid.arrange(g1, g2, g3, ncol = 3)

```

The plots above of the confusion matrices, show how the Random Forest model performance (model 3) out performs the Classification Tree model. It can also be seen that Model 1 can not predict Classe type "D". 

## Summary and Conclusions
The random Forest model out performed the Classification Tree and Pruned tree models by leaps and bounds with an accuracy over 99%, concluding for this data set it is the best model selection. An important note is that the results are only based on the data set that the models were trained on and it is expected the results could be different for data collected under different circumstances and conditions, such as new individuals who perform the exercises. 

